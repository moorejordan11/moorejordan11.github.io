{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Client Report - Project 4\"\n",
        "subtitle: \"Course DS 250\"\n",
        "author: \"Jordan Moore\"\n",
        "format:\n",
        "  html:\n",
        "    self-contained: true\n",
        "    page-layout: full\n",
        "    title-block-banner: true\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    toc-location: body\n",
        "    number-sections: false\n",
        "    html-math-method: katex\n",
        "    code-fold: true\n",
        "    code-summary: \"Show the code\"\n",
        "    code-overflow: wrap\n",
        "    code-copy: hover\n",
        "    code-tools:\n",
        "        source: false\n",
        "        toggle: true\n",
        "        caption: See code\n",
        "execute: \n",
        "  warning: false\n",
        "    \n",
        "---"
      ],
      "id": "78453eee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import plotly_express as px\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree \n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.cluster import BisectingKMeans, KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "id": "aaae8479",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Elevator pitch\n",
        "- We try to predict everything possible because it also makes our lives easier. How can we use analytics to our advantage and practice to see if we can predict past data and find what works and what doesn't work. We used prediction analysis to predict that you would read this report.\n"
      ],
      "id": "f650149a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dwellings_ml = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")"
      ],
      "id": "1f9abf50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1\\|Task 1\n",
        "__Create 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.__\n",
        "-In the first chart we see that has size of different aspects like living space size that the liklihood of the house being built after 1980 increases. The second chart shows the correlation that as basement size increase then the predictability of the house being built after 1980 increases as well.\n"
      ],
      "id": "5efd09f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "h_subset = dwellings_ml.filter(\n",
        "    ['livearea', 'finbsmnt', 'basement', \n",
        "    'nocars', 'numbdrm', 'quality_C', 'before1980',\n",
        "    'stories', 'yrbuilt']).sample(500)\n",
        "chart = px.scatter_matrix(h_subset,\n",
        "    dimensions=['livearea', 'finbsmnt', 'basement'],\n",
        "    color='before1980'\n",
        ")\n",
        "chart.update_traces(diagonal_visible=False)\n",
        "chart.update_layout(\n",
        "    title_text=\"Scatter Matrix of Living Area, Finished Basement, and Total Basement\",\n",
        "    title_x=0.5,\n",
        "    annotations=[\n",
        "        dict(\n",
        "            text=\"Relationships between year built and sizes of the three categories with yellow being before 1980\",\n",
        "            showarrow=False,\n",
        "            xref=\"paper\",\n",
        "            yref=\"paper\",\n",
        "            x=0.5,\n",
        "            y=-0.2,  # This is for adjusting caption location\n",
        "            font=dict(size=12)\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "chart.show()"
      ],
      "id": "4a148b61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = dwellings_ml[['basement']].values\n",
        "y = dwellings_ml['yrbuilt'].values\n",
        "\n",
        "# Splits the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scales the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Creates a range of X values for prediction\n",
        "T = np.linspace(X.min(), X.max(), 500).reshape(-1, 1)\n",
        "T_scaled = scaler.transform(T)\n",
        "\n",
        "# Sets up the plot\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
        "# Nearest neighbors amount can be switched around. 5 seemed to be the most efficient model.\n",
        "n_neighbors = 4\n",
        "\n",
        "for i, weights in enumerate([\"uniform\", \"distance\"]):\n",
        "    knn = KNeighborsRegressor(n_neighbors, weights=weights)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(T_scaled)\n",
        "\n",
        "    # Adds the trendline\n",
        "    z = np.polyfit(X_train.flatten(), y_train, 1)\n",
        "    p = np.poly1d(z)\n",
        "    trendline = p(T.flatten())\n",
        "\n",
        "    axs[i].scatter(X_train, y_train, color=\"darkorange\", label=\"data\", alpha=0.1)\n",
        "    axs[i].plot(T, y_pred, color=\"navy\", label=\"prediction\")\n",
        "    axs[i].plot(T, trendline, color=\"red\", linestyle=\"--\", label=\"trendline\")\n",
        "    axs[i].set_xlabel('Basement Size')\n",
        "    axs[i].set_ylabel('Year Built')\n",
        "    axs[i].legend()\n",
        "    axs[i].set_title(f\"KNeighborsRegressor (k = {n_neighbors}, weights = '{weights}')\")\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.suptitle(\"KNN Regression: Basement Size vs Year Built\", fontsize=16, y=1.02)\n",
        "\n",
        "# Adds a caption\n",
        "caption = (\"Figure 1: KNN Regression models comparing uniform and distance-based weights. \"\n",
        "           \"The plots show the relationship between basement size and year built, \"\n",
        "           \"with the original data points (orange), KNN prediction (navy), and linear trendline (red dashed).\")\n",
        "\n",
        "fig.text(0.5, -0.05, caption, wrap=True, horizontalalignment='center', fontsize=12)\n",
        "plt.show()"
      ],
      "id": "6dff81b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2 \\|Task 2\n",
        "__Build a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.__\n",
        "-90% accuracy was achieved as the need to find the most efficient amount of features and which ones were used. As we don't want to use too many or not enough as we'll see later on why that matters. These are the metrics explained: \n",
        "Precision: How many of the predicted positives are actually positive.\n",
        "Recall: How many of the actual positives were correctly identified.\n",
        "F1-score: The harmonic mean of precision and recall.\n",
        "Support: The number of instances for each class.\n",
        "We see our machine learning was efficient for both predictions and actual values.\n"
      ],
      "id": "12500ce9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %%\n",
        "X_pred = dwellings_ml.filter(regex='arcstyle_ONE-STORY|gartype_Att|quality_C|livearea|basement|stories|numbdrm')\n",
        "\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .34, random_state = 76)  \n",
        "\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=10)\n",
        "clf = clf.fit(X_train, np.ravel(y_train))\n",
        "y_pred2 = clf.predict(X_test)\n",
        "\n",
        "# %%\n",
        "print(metrics.classification_report(y_pred2, y_test))"
      ],
      "id": "5075a9d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3 \\|Task 3\n",
        "__Justify your classification model by discussing the most important features selected by your model. This discussion should include a feature importance chart and a description of the features.__\n",
        "-The features we used was the top 7 from this list. This chart shows the top features the prediction functions use in predicting whether a house was built before or after 1980. Live area in the Random Forest Classifier function was the most heavily weighted for it's predictions on whether it was before or after 1980.\n"
      ],
      "id": "ce6219a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %%\n",
        "df_features = pd.DataFrame(\n",
        "    {'f_names': X_train.columns, \n",
        "    'f_values': clf.feature_importances_}).sort_values('f_values', ascending = False)\n",
        "\n",
        "#%%\n",
        "chart = px.bar(df_features.head(14),\n",
        "    x='f_values', \n",
        "    y='f_names'\n",
        ")\n",
        "# The chart is saying that it made it's decision based off of the top row.\n",
        "chart.update_layout(yaxis={'categoryorder':'total ascending'})\n",
        "chart.update_layout(\n",
        "    title_text=\"Weighted values for Random Forest\",\n",
        "    title_x=0.5,\n",
        "    annotations=[\n",
        "        dict(\n",
        "            text=\"The highest weighted values used by Random Forest prediction with the variables on the left and the weight on the bottom\",\n",
        "            showarrow=False,\n",
        "            xref=\"paper\",\n",
        "            yref=\"paper\",\n",
        "            x=0.5,\n",
        "            y=-0.2,  # This is for adjusting caption location\n",
        "            font=dict(size=12)\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "chart.show()"
      ],
      "id": "5989dc7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4 \\|Task 4\n",
        "__Describe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.__\n",
        "-AUC Score (Area Under the Curve), this is a measure of the ability of the classifier to distinguish between classes.The AUC score ranges from 0 to 1, where 1.0 represents a perfect classifier and a 0.5 represents a classifier that's no better than random guessing. The closer the curve is towards the top left corner the better it is at predicting."
      ],
      "id": "3aacf5d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% \n",
        "#roc curve\n",
        "# Makes it so as you add more does it really add more?\n",
        "metrics.RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
        "fig = plt.gcf()\n",
        "caption = (\"Figure 1: Receiver Operating Characteristic (ROC) Curve. \"\n",
        "           \"This plot shows the performance of the classifier at all classification thresholds. \"\n",
        "           \"The diagonal line represents random guessing, while the curve represents the model's performance. \"\n",
        "           \"A higher Area Under the Curve (AUC) indicates better classification performance.\")\n",
        "\n",
        "plt.figtext(0.5, -0.05, caption, wrap=True, horizontalalignment='center', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(bottom=0.2)\n",
        "\n",
        "plt.show()"
      ],
      "id": "ea6a9027",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confusion matrix based on Random Forest Classifier shows a high percentage of predictions were correct as it predicted correctly 57% of the actual houses correclty and avoided 33% of the incorrect predictions. Confusion matrix explained: True Positive (TP): Predicted 1 (positive) and actually 1 (positive). True Negative (TN): Predicted 0 (negative) and actually 0 (negative). False Positive (FP): Predicted 1 (positive) but actually 0 (negative). False Negative (FN): Predicted 0 (negative) but actually 1 (positive).\n"
      ],
      "id": "c012a9bb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cm = confusion_matrix(y_test, y_pred2)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "id": "09bb06e7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 5 \\|Task 5\n",
        "__Repeat the classification model using 3 different algorithms. Display their Feature Importance, and Decision Matrix. Explian the differences between the models and which one you would recommend to the Client.__\n",
        "-There's different methods used to try and predict what we want. Below we see the comparison of numbers between using a Linear Regression model, Random Forest, or a Nearest Neighbor model. I explain below what the numbers mean. The Random Forest predicts more accurately then Linear Regression or Nearest Neighbor.\n",
        "\n",
        "Based on Random Forest our data shows: \n",
        "MAE: Your model, on average, predicts the year a house was built with an error of around 8 months, which is relatively small considering the range of years.\n",
        "MSE: This metric is lower, indicating that while there are some larger errors, they do not significantly affect the overall average error.\n",
        "R-squared (R2): The R2 score of approximately 0.640 suggests that the model captures a moderate amount of variation in predicting the year a house was built based on the selected features using over 50% of the data.\n",
        "An R2 score of approximately 0.640 means that the model explains about 64% of the variance in the year built based on the features used. A higher R2 score closer to 1 indicates a better fit.\n",
        "Comparison for all three is below.\n"
      ],
      "id": "dc67831c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Random Forest data\n",
        "dwellings_ml_extra = dwellings_ml.copy()\n",
        "\n",
        "# Splits data into features (X) and target (y)\n",
        "X = dwellings_ml.filter(regex='arcstyle_ONE-STORY|gartype_Att|quality_C|livearea|basement|stories|sprice|netprice|numbdrm|finbsmnt')\n",
        "y = dwellings_ml.filter(regex = \"before1980\").values.ravel()\n",
        "\n",
        "# Splits data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializes and train the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicts on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluates the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae:.3f}')\n",
        "print(f'Mean Squared Error (MSE): {mse:.3f}')\n",
        "print(f'R-squared (R2): {r2:.3f}')"
      ],
      "id": "537287a4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Linear Regression data\n",
        "dwellings_ml_extra = dwellings_ml.copy()\n",
        "\n",
        "# Splits data into features (X) and target (y)\n",
        "X = dwellings_ml.filter(regex='arcstyle_ONE-STORY|gartype_Att|quality_C|livearea|basement|stories|sprice|netprice|numbdrm|finbsmnt')\n",
        "y = dwellings_ml.filter(regex = \"before1980\")\n",
        "\n",
        "# Splits data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializes and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicts on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluates the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "# rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae:.3f}')\n",
        "print(f'Mean Squared Error (MSE): {mse:.3f}')\n",
        "# print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "print(f'R-squared (R2): {r2:.3f}')"
      ],
      "id": "9c22202c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Nearest Neighbors data\n",
        "\n",
        "X = dwellings_ml.filter(regex='arcstyle_ONE-STORY|gartype_Att|quality_C|livearea|basement|stories')\n",
        "y = dwellings_ml['before1980']\n",
        "\n",
        "# Splits data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=4)\n",
        "\n",
        "# Trains the model on the training data\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "# Predicts on the test set\n",
        "y_pred = knn_regressor.predict(X_test)\n",
        "\n",
        "# Evaluates the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae:.3f}')\n",
        "print(f'Mean Squared Error (MSE): {mse:.3f}')\n",
        "print(f'R-squared (R2): {r2:.3f}')"
      ],
      "id": "2843fa19",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}